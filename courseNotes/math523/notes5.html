<html>
<head>
	<title>GenLin</title>
	<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
	<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	  MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
	</script>
	<script type="text/javascript"
	  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
	<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
	<link rel="stylesheet" type="text/css" href="notes.css">
	
</head>
<body>
<div id="menu"></div>
<h2>Estimate</h2>
<a name="algo"></a><h3>Algorithms for maximizing the likelihood</h3><h4>Newton-Raphson (N-R)</h4>
<ol>
	<li>Start with an initial approximation of the solution</li>
	<li>Approximate the target function in a neighborhood of the initial approximate solution by a second-degree polynomial</li>
	<li>Find the maximum of the polynomial and set that as the new approximate solution</li>
	<li>Repeat to obtain a sequence of approximations</li>
</ol>
Need to construct the Hessian matrix $H$ with entries
\begin{equation*}
	h_{ab} = \frac{\partial^2 L(\beta)}{\partial\beta_a\beta_b}
\end{equation*}
which is $(p+1)\times (p+1)$, and the vector 
\begin{equation*}
	u = \left(\frac{\partial L(\beta)}{\partial\beta_0},\dots, \frac{\partial L(\beta)}{\partial\beta_p}\right)
\end{equation*}
Leading to the following iteration update
\begin{equation*}
	\beta^{(t+1)} = \beta^{(t)} - (H^{(t)})^{-1}u^{(t)}
\end{equation*}
if $H^{(t)}$ is non-signular.
<br><br>
The algorithm in general is not guaranteed to obtain the global maximum, as it could encounter local maximum, minimum or saddlepoint. If the design matrix is full-rank, then for many GLM's the log-likelihood will be a strictly concave function, in which case the ML estimates will exist and will be unique under very general conditions. As a bonus, the convergence of the estimates will be quite fast.
<h4>Fisher Scoring</h4>
One modification to the N-R algorithm is to replace the Hessian with the expected value of the Hessian, referred to as the expected information matrix. 
\begin{equation*}
	\mathcal{I}^{(t)}_{ab} = -E\left(\frac{\partial^2 L(\beta)}{\partial\beta_a\partial\beta_b}\right)
\end{equation*}
evaluated at $\beta^{(t)}$. The updates becomes 
\begin{equation*}
	\beta^{(t+1)} = \beta^{(t)} + (\mathcal{I}^{(t)})^{-1}u^{(t)}
\end{equation*}
or 
\begin{equation*}
	\mathcal{I}^{(t)}\beta^{(t+1)} = \mathcal{I}^{(t)}\beta^{(t)} + u^{(t)}
\end{equation*}

Advantage of this over N-R:
<ol>
	<li>$(\mathcal{I}^{(t)})^{-1}$ is computed as a by-product of the estimation algorithm and is the estimated variance covariance matrix of $\hat{\beta}$.</li>
	<li>It doesn’t directly require computing the matrix of second derivatives (as we can take the expected value of the outer product of the gradient).</li>
	<li>It tends to be more robust than N-R because $\mathcal{I}^{(t)}$ doesn’t depend directly on the data observed.</li>
	<li>However, N-R does have better theoretical convergence properties in that it will converge quadratically in a neighborhood of the true maximum, whereas Fisher scoring is merely sublinear.
	</li>
</ol>
<br><br>
Fisher scoring algorithm is generally called an iteratively reweighted least squares (IRLS) algorithm. At convergence, 
\begin{equation*}
	\hat{\beta} = (X^\top\hat{W}X)^{-1} \hat{W}\hat{z}
\end{equation*}
for 
\begin{equation*}
	\hat{z} = X\hat{\beta} + D^{-1}(y-\hat{\mu})
\end{equation*}
<h4>For Canonical Link Function</h4>
For the canonical link function ($\phi = $ constant), the N-R and Fisher scoring algorithms are identical for GLMs, and 
\begin{equation*}
	H_W = -\mathcal{I}
\end{equation*}
Note that $b(\theta)$ is a convex function
($b''(\theta) = Var(y_i)/a(\phi)$), and can be shown to not be a saddlepoint in multiple dimensions (as the second partial derivatives are the covariance of the sample statistics). Thus, an advantage of the canonical link in that the log-likelihood must be concave in $\theta$, because its log contains the sum of $−b(\theta)$ (a concave function) and is linear in $\theta$. 
<a name="disp"></a><h3>Dispersion Parameter</h3>
When $\phi$ is not constant, it is estimated by matching the moments of the individual $y_i$ and to solve the system of equations
\begin{equation*}
	\sum_{i=1}^n\left[\frac{(y_i-\mu_i)^2}{\phi Var(\mu_i)} - 1\right]=0
\end{equation*}
This leads to 
\begin{equation*}
	\hat{\phi} = \frac{\sum_{i=1}^n(y_i-\hat{\mu}_i)^2}{n}
\end{equation*}
<h4>From Breslow</h4>
Breslow (1984) discusses potential bias for this estimator for large $p$ relative to $n$, so he suggested to solve insdead
\begin{equation*}
	\sum_{i=1}^n\left[\frac{(y_i-\mu_i)^2}{\phi Var(\mu_i)} - \frac{n-p}{n}\right]=0
\end{equation*}
leading to
\begin{equation*}
	\hat{\phi}_{MM} = \frac{\sum_{i=1}^n(y_i-\hat{\mu}_i)^2}{n-p}
\end{equation*}
Note that this procedure uses scaled values of the difference between observed and fitted values, giving it the form of a squared Pearson residual.

<h4>Using Deviance Residual</h4>
Any adequate model $M$ under small-dispersion asymptotics, the deviance for that model satisfies:

\begin{equation*}\frac{D_M}{\phi} \sim \chi_{n-p}^2\end{equation*}

due to the equivalence between the LRT statistic and scaled deviance. Therefore, the dispersion parameter can be estimated with 
\begin{equation*}
	\hat{\phi}_{D_M} = \frac{D_M}{n-p}
\end{equation*}
as the mean of a $\chi^2_{n-p}$ random variable is $n - p$. 
<br><br> 
$\hat{\phi}_{D_M}$ is providing an estimate of the extent to which the deviance exceeds the asymptotic mean under $\phi = 1$. $\hat{\phi}_{D_M}$ and $\hat{\phi}_{MM}$ will often be close due to the relationship between the deviance and Pearson residuals.

<h4>Using Score Equation</h4>
The dispersion can be estimated by solving the score equation for the MLE of $\phi$. The dispersion and regression coefficients are orthogonal, so this can be done after Fisher scoring has produced MLE's $\hat{\beta}$. 
<br><br>
The key advantage of this is that if N-R algorithm is used to find $\hat{\phi}_{ML}$, then one can obtain a standard error for $\hat{\phi}_{ML}$, whereas for the other two they must be derived by hand using methods for asymptotics of M-estimators (i.e. estimators which are solutions to systems of estimating equations). The key disadvantage is that the specification of the distribution is required to be correct for consistency of $\hat{\phi}_{ML}$, whereas the other two estimators only require that the first two moments of the response are correctly specified.

<h4>Deviance Test for Unknown Dispersion</h4>
Note that with a consistently estimated
dispersion parameter, 
\begin{equation*}
	\frac{D_0-D_1}{\hat{\phi}_{D_M}} \rightarrow_d \chi^2_{p_1-p_0}
\end{equation*}
where $M_0$ is a model nested withing $M_1$. $\hat{\phi}_{D_M}$ is in practive from $M_1$. However, the estimate of the dispersion parameter need not be from either model $M_0$ or $M_1$ and could be from a more complex, larger model that has been deemed to be adequate.

<a name="qasi"></a><h3>Quasi-Likelihood</h3>
Quasi-likelihood estimation is an approach that specifies a link function and linear predictor $g(\mu_i) = \sum_{j}\beta_jx_{ij}$ , but does not assume a probability distribution for $y_i$. Assumptions are only made about the mean-variance relationship, i.e.: replace $Var(y_i)$ with any function $Var(\mu_i)$ which is appropropriate for the situation. Then, with dispersion
\begin{equation*}
	Var(y_i) = \phi Var^*(\mu_i)
\end{equation*}
$\phi >1$ represents overdispersion relative to the original distribution. Note that adding $\phi$ to the variance term yields identical estimate of $\hat{\beta}$ and so $\hat{\mu}_i$ also remains the same.
But the assimptotic variance of $\hat{\beta}$ becomes
\begin{equation*}
	Var{\hat{\beta}} = (X^\top WX)^{-1}\phi
\end{equation*}
<h4>Estimate Dispersion</h4>
$\phi$ can be estimated ising the moment estimator based on the generalized Pearson statistc for the origin model
\begin{equation*}
	X^2 = \sum_{i=1}^n\frac{y_i - \hat{\mu}_i)^2}{Var^*(\mu_i)}
\end{equation*}
by taking
\begin{equation*}
	\hat{\phi} = \frac{X^2}{n-(p+1)}
\end{equation*}
The advantages of maximum likelihood are efficient estimator and clearly interpretable distribution. The advantages of quasi-likelihood are complete freedom in choosing the form of the variance function and often more computational stability.
<h4>Overdispersion</h4>
The most common use of quasi-likelihood is for overdispersed Poisson regression models, which assumes that $Var(\mu_i) = \mu_i$.

<a name="nbin"></a><h3>Negative-Binomial</h3>
There are two choices for overdispersed count regression models:
<ul><li>Maximum likelihood via a negative-binomial model</li>
<li>Quasi-likelihood model based only on the choice of variance and mean functions</li>
</ul>
For negative-binomial,
\begin{align*}
	&f_{Y_i}(y_i|\mu_i, \gamma_i) = {y_i+\alpha_i-1 \choose y_i}\left(\frac{\mu_i}{\alpha_i+\mu_i}\right)^{y_i}\left(\frac{\alpha_i}{\alpha_i+\mu_i}\right)^{\alpha_i}\\
	&b(\theta_i) = -\log(1-\exp(\theta_i))\\
	&a(\phi) = 1/\alpha_i\\
	&\dot{b}(\theta_i) = \frac{\mu_i}{\alpha_i}\\
	&E[y_i] = \dot{b}(\theta_i)/a(\phi) = \mu_i\\
	&Var[y_i] = \ddot{b}(\theta_i)/a(\phi) = \mu_i + \mu_i^2/\alpha_i
\end{align*}
Note that as $\alpha_i \rightarrow \infty$, $Var[y_i] \rightarrow \mu_i$, which results the form of the Poisson model. 
<br><br>
The response variables is assumed to have different means, but the same dispersion parameter, $1/\alpha_i = 1/\alpha = \gamma$. This can be justified under the Gamma-Poisson mixture by noting that the coefficient of variation for the heterogeneous Poisson means then have constant coefficient of variation, $\sqrt{Var(\lambda)}/E(\lambda) = \sqrt{\gamma}$. Under this parametrization, overdispersion relative to the Poisson is indicated by $\gamma > 0$.

<h4>Link Function</h4>
The canonical link 
\begin{equation*}
	g(\mu_i) = \log\left(\frac{\mu_i}{\alpha_i+\mu_i}\right)
\end{equation*}
is rarely used in practive, in order to compare the nagative binomial model results to the Poisson. Generally software will by default choose the log link $g(\mu_i) = \log(\mu_i)$. 
<br><br>
The score equation built from this lead to orthogonal and thus asymptotically independent $\beta$ and $\gamma$. This means the true large sample standard error $\hat{\beta}_j$ is the same, whether $\lambda$ is known or estimated.

<a name="ivbd"></a><h3>Dispersion in Bernoulli Data</h3>
We can also use a quasi-likelihood approach to accommodate additional variance beyond what the usual binomial regression allows.  
<br><br>
Let $y_{i1}, \dots, y_{in_n}$ be $n_i$ trials for observation $i$. Let $\pi_i = P(Y_{ij} = 1)$ and $y_i = \sum_{j}y_{ij}/n_i$. IF the trials are independent, then $n_iy_i\sim \mbox{Binomial}(n_i, \pi_i)$ with variance $Var(y_i) = Var(\pi_i) = \pi_i(1-\pi_i)/n_i$. 
<br><br>
The additional variance can occur in two different ways. 
<h4>Inflated Variance</h4>
Unmeasured covariates are associated to the reponse probabilities $\pi_i$, which causes
two different observations, $i$ and $j$ with identical observed covariate vectors to have two different true probabilities of
success. This would cause observation of additional heterogeneity in the sample proportions, $y_i$, which would not be accomodated by the usual binomial variance. An inflated-variance quasi-likelihood approach would use the variance function
\begin{equation*}
	Var(\pi_i) = \phi\frac{\pi_i(1-\pi_i)}{n_i}
\end{equation*}
$\phi$ can be estimated using Pearson's goodness of fit 
\begin{equation*} 
	\hat{\phi} = X^2/(n-[p+1])
\end{equation*}
where 
\begin{equation*}
	X^2 = \sum_i \frac{(y_i-\hat{\pi}_i)^2}{[\hat{\pi}_i(1-\hat{\pi}_i)]/n_i}
\end{equation*}

This model can be effective for modelling overdispersion because of its simplicity and interpretability. However, it cannot be used when $n_i = 1$, in which case $E(Y^2_i) = E(Y_i) = \pi_i$ and $Var(y_i) = \pi_i(1 - \pi_i)$ (with no possibility for $\phi \neq 1$).

<h4>Correlated Bernoulli Responses</h4>
Trials within the $i$th observation can be  very strongly positively dependent. Data can also be sampled in clusters. 
<br><br>
Let $\rho = corr(y_{is}, y_{it})$ for $s\neq t$. Then 
\begin{equation*}
	Var(y_i) = (1+\rho(n_i-1))\frac{\pi_i(1-\pi_i)}{n_i}
\end{equation*}
$\rho = 0$ is the case of independent Binomial sampling variance. If $\rho > 0$, variance is larger than independence and if $\rho < 0$, variance is smaller. Note that the inflated variance model cannot be obtained as a special case of this correlated data model, except when the $n_i = n^* \ \forall i$, and this model handles $n_i = 1$ with no problem.
<br><br>
The quasi-likelihood model this this assumes that 
\begin{equation*}
	Var(\pi_i) = [1+\rho(n_i-1)]\pi_i(1-\pi_i)/n_i
\end{equation*}
as long as $|\rho| \leq 1$. The score function for this can be solved using Williams's algorithm (proposed in 1982).

<a name="beta"></a><h3>Beta-Binomial</h3>
For correlated Bernoulli data, a parametric alternative to the quasi-binomial model is to use a beta-binomial mixture model. Similar to the Gamma-Poisson mixture, a parametric distribution for the $\pi_i$ is assimed and then ML is used to obtain parameter estimates. Assume $\pi \sim \mbox{Beta}(\alpha_1, \alpha_2)$.
\begin{equation*}
	f(\pi;\alpha_1,\alpha_2) = \frac{\Gamma(\alpha_1 + \alpha_2)}{\Gamma(\alpha_1)\Gamma(\alpha_2)}\pi^{\alpha_1-1}(1-\pi)^{\alpha_2-1}
\end{equation*}
The Beta distribution restricts $\pi$ within (0, 1) and provides a variety of shapes, including a uniform distribution $(\alpha_1 = \alpha_2 = 1)$, unimodal symmetric $(\alpha_1 = \alpha_2 > 1)$ and bi-modal, U-shaped distributions $(\alpha_1 < 1, \alpha_2 < 1)$. 
<br><br> However, Beta-binomial distribution does not belong to the exponential dispersion family of distributions, even for a known $\theta$. So, any link function can be used, although the logit is probably the most commonly used. This means that one only can use maximum likelihood (via N-R, for example), but one cannot rely on the generalized linear model framework. Even if the linear predictor is correct, if the data are not truly Beta-binomial, then $\hat{\beta}$ will not be consistent.
<div id="footer"></div>

<script src="notes.js"></script>
</body>
</html>