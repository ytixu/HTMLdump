<html>
<head>
	<title>GenLin</title>
	<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
	<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	  MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
	</script>
	<script type="text/javascript"
	  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
	<style>
		body{
			font-family: 'Open Sans', sans-serif;
			margin:10%;
			font-size: 20px;
		}
		a{
			color: #333333;
			text-decoration: none;
		}
		a:hover, div#scrollTop:hover{
			color: #aaaaaa;
			cursor: pointer;
		}
		div#scrollTop{
			position:fixed;
			bottom:10px;
			right:10px;
			text-align: center;
			color: #333333;
		}
		h1 {
        	counter-reset: h2counter;
	    }
	    h2 {
        	counter-reset: h3counter;
	    }
	    h3 {
        	counter-reset: h4counter;
	    }
	    h2:before {
	        content: counter(h2counter) ".\0000a0\0000a0";
	        counter-increment: h2counter;
	    }
	    h3:before {
	        content: counter(h2counter) "." counter(h3counter) ".\0000a0\0000a0";
	        counter-increment: h3counter;
	    }
	    ol { 
	    	counter-reset: item 
	    }
		ol li { 
			display: block 
		}
		ol li:before { 
			content: counters(item, ".") "."; 
			counter-increment: item; 
			padding-right:10px; 
			margin-left:-20px;
		}
	    .index{
	    	font-size: 30px;
	    }
	</style>
	<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
	
</head>
<body>
<h1>Generalized Linear Models</h1>
<hr>
<span class="index">Content</span>
<ol>
	<li><a href="#efam">Exponential Familiy (EF)</a>
		<ol>
			<li><a href="#edfa">Exponential Disperson Family (EDF)</a></li>
			<li><a href="#jorg">J&#248;rgensen's Exponential Disperson Family Model</a></li>
			<li><a href="#mlee">Maximum Likelihood Estimates for EDF</a></li>
			<li><a href="#algo"></a></li>
			<li><a href="#fish">Fisher Information</a></li>
			<li><a href="#delt">Delta Method</a></li>
			<li><a href="#robo">Robustness</a></li>
		</ol></li>
	<li><a href="#magf">Model Assessment and Goodness-of-Fit</a>
		<ol>
			<li><a href="#LRT">Likelihood-Ratio Test (LRT)</a></li>
			<li><a href="#wald">Wald's Test</a></li>
			<li><a href="#rao">Rao's Score Test</a></li>
			<li><a herf="#gofi">Goodness-of-Fit</a></li>
			<li><a herf="#resi">Residual</a></li>
		</ol></li>
	<li><a href="#infr">Inference</a>
		<ol>
			<li><a href="#cfit">Confidence Interval</a></li>
			<li><a href="#plrt">Profile Log-Likelihood Ratio Test (PLRT)</a></li>
		</ol></li>
	<li><a href="#egmd">Relevant Models</a>
		<ol>
			<li><a href="#logm">Logistic Model</a></li>
			<li><a href="#prom">Probit Model</a></li>
			<li><a href="#poim">Poission Model</a></li>
		</ol></li>
</ol>
<hr>
<!--<a name="efam"></a><h2>Exponential Familiy (EF)</h2>
Consider $Y_i$ with distribution belonging to the linear exponential family, 
\begin{equation*}
	f_{Y_i}(Y_i|\theta) = \exp(-b(\theta)+ \theta y_i + c(y_i))
\end{equation*}
where $b(\theta)$ is a function of the parameters only and $c(y_i)$ is a function of the data only.
Assuming that the first 2 partial derivatives exist for all $\theta$ in the define support of the paprameter space, let
\begin{align*}
	&I(\theta;y_i) = \log(f_{Y_i}(Y_i|\theta))\\
	&\dot{I}(\theta;y_i) = \frac{\partial I(\theta;y_i)}{\partial\theta}\\
	&\ddot{I}(\theta;y_i) = \frac{\partial^2 I(\theta;y_i)}{\partial\theta^2}
\end{align*}
In particular 
\begin{equation}
	\label{eq:efexpand}
	\dot{I}(\theta;y_i) = \frac{\partial}{\partial \theta} (-b(\theta)+ \theta y_i + c(y_i)) = -\dot{b}(\theta) + y_i
\end{equation}
and 
\begin{equation}
	\label{eq:efexpandd}
	\ddot{I}(\theta;y_i) = \frac{\partial}{\partial \theta}(-\dot{b}(\theta) + y_i) = -\ddot{b}(\theta)
\end{equation}
Here are some interesting properties.
<ol><li>
\begin{equation}
  \label{eq:efexp}
  E\left[\dot{I}(\theta;y_i)\right] = 0
\end{equation}
</li><li>
	Let $E\left[m(y_i, \theta)\right] = 0$ for some function $m$, then 
\begin{equation*}
	E\left[\frac{\partial m(y_i, \theta)}{\partial \theta'}\right] = 
	-E\left[m(y_i, \theta)\frac{\partial I(\theta;y_i)}{\partial\theta'}\right]
\end{equation*}
</li><li>
	In the special case when $m(y_i, \theta) = \dot{I}(\theta;y_i)$,
	\begin{equation}
	\label{eq:efvar}
	E\left[\ddot{I}(\theta;y_i)\right] = 
	-E\left[(\dot{I}(\theta;y_i))^2\right]
\end{equation} 
</li>
</ol>
TODO: PROOFS 
Using these result, we can get the followings
<h4>Expectation</h4>
Combining equation \eqref{eq:efexpand} and \eqref{eq:efexp}, 
\begin{align*}
	E\left[\dot{I}(\theta;y_i)\right] = E[-\dot{b}(\theta) + y_i] = -\dot{b}(\theta) + E[y_i] = 0
\end{align*}
Therefore, 
\begin{equation*}
\dot{b}(\theta) = E[y_i]
\end{equation*}
<h4>Variance</h4>
Combining equation \eqref{eq:efexpandd} and \eqref{eq:efvar}, 
\begin{align*}
	-E\left[\ddot{I}(\theta;y_i)\right] = E\left[(\dot{I}(\theta;y_i))^2\right] = E[(-\dot{b}(\theta) + y_i)^2] = Var(y_i)
\end{align*}
Hence, 
\begin{equation*}
\ddot{b}(\theta) = Var(y_i)
\end{equation*}
Seeing that $b(\theta)$ can get the moments of $Y_i$, this function is thus given the name <i>cumulent function</i>. 
<a name="edfa"></a><h3>Exponential Disperson Family (EDF)</h3>
Consider $Y_i$ with distribution belonging to the linear exponential dispersion family, 
\begin{equation*}
	f_{Y_i}(Y_i|\theta, \phi) = \exp\left(\frac{\theta y_i-b(\theta)}{a(\phi)} + c(y_i, \phi)\right)
\end{equation*}
where $\phi$ modifies the disperson in a multiplicative way. In this case, the expectation and variance of $Y_i$ are respectively
\begin{align*}
&E[Y_i] = \dot{b}(\theta)\\
&Var[Y_i] = \ddot{b}(\theta)a(\phi)
\end{align*}
<a name="jorg"></a><h3>J&#248;rgensen's Exponential Disperson Family Model</h3>
J&#248;rgensen showed that there do not exist many discrete distribution which can be expressed as the continuous exponential dispersion family ditribution. So he proposed 
\begin{equation*}
	f_{Y_i}(Y_i|\theta, \phi) = \exp\left(\theta y_i-\frac{b(\theta)}{a(\phi)} + c(y_i, \phi)\right)
\end{equation*}
wich expectation and variance 
\begin{align*}
&E[Y_i] = \dot{b}(\theta)/a(\phi)\\
&Var[Y_i] = \ddot{b}(\theta)a(\phi)
\end{align*}
<a name="mlee"></a><h3>Maximum Likelihood Estimates for EDF</h3>
Let $Y_1,\dots, Y_n \overset{i.i.d.}{\sim}f_{Y_i}(y_i)$. In the case when $\phi$ is known, 
\begin{align*}
\mathcal{L}(\theta;y) &= \sum_{i=1}^n\left(\frac{\theta y_i-b(\theta)}{a(\phi)} + c(y_i, \phi)\right)\\
\frac{\partial}{\partial \theta}\mathcal{L}(\theta;y) &= \sum_{i=1}^n\frac{y_i-\dot{b}(\theta)}{a(\phi)} = 0\\
\dot{b}(\theta) &= \frac{1}{n}\sum_{i=1}^ny_i
\end{align*}
The MLE for one parameter EF is also the method of moment estimator for the mean.
<br><br>
In the case when $phi$ is unknown,
\begin{align*}
\frac{\partial}{\partial \phi}\mathcal{L}(\theta;y) &= -\frac{\dot{a}(\phi)}{(a(\phi))^2}\sum_{i=1}^n(y_i-b(\theta)) + \sum_{i=1}^n\dot{c}(y_i, \phi) = 0\\
\end{align*}
and we stil have $\dot{b}(\theta) = \frac{1}{n}\sum_{i=1}^ny_i$. If $a(\phi) = \phi$, then 
\begin{equation*}
	\sum_{i=1}^n(y_i - b(\theta)) = \phi^2\sum_{i=1}^n\dot{c}(y_i, \phi)
\end{equation*}
<h4>Link Function</h4>
To connect the covariates with the mean, we choose a mapping $g$ that is linear and invertible. 
\begin{equation*}
	\eta_i = g(\mu_i) = \sum_{j=0}^p\beta_jx_{ij}
\end{equation*}
For $\theta_i$ such that $\dot{b}(\theta_i) = \mu_i$, if $g(\mu_i) = \theta_i$ then $g$ is a <i>canonical link function</i>.
<h4>Score Equation</h4>
To get MLE for $\beta_j$, let
\begin{equation*}
	l(\beta_j;y) := \frac{\partial}{\partial \beta_j} \mathcal{L}(\beta_j;y) = \sum_{i=1}^n\frac{\partial}{\partial \beta_j} \mathcal{L}_i(\beta_j;y_i) = 0
\end{equation*}
Using the chain rule, 
\begin{equation*}
	\frac{\partial \mathcal{L}_i}{\partial \beta_j} = \frac{\partial \mathcal{L}_i}{\partial \theta_i}\frac{\partial \theta_i}{\partial \mu_i}\frac{\partial \mu_i}{\partial \eta_i}\frac{\partial \eta_i}{\partial \beta_j}
\end{equation*}
where 
\begin{align*}
	&\frac{\partial \mathcal{L}_i}{\partial \theta_i} = \frac{y_i-\dot{b}(\theta)}{a(\phi)} = \frac{y_i-\mu_i}{a(\phi)}\\
	&\frac{\partial \mu_i}{\partial \theta_i} = \frac{\partial \dot{b}(\theta)}{\partial \theta_i} = \ddot{b}(\theta_i) = \frac{Var(y_i)}{a(\phi)}\\
	&\frac{\partial \eta_i}{\partial \beta_j} = x_{ij}
\end{align*}
Ergo,
\begin{equation*}
	l(\beta_j;y) = \sum_{i=1}^n \frac{y_i-\dot{b}(\theta)x_{ij}}{Var(y_i)}\times \frac{\partial \mu_i}{\partial \eta_i} = 0
\end{equation*}
where $\frac{\partial \mu_i}{\partial \eta_i}$ depends on the link function. This is the score equation.
<h4>Matrix format</h4>
Let $V$ be a diagonal matric of response variance, i.e.: $V_{ii} = Var(y_i)$, and let $D$ is another diagonal matrix with $D_{ii} = \frac{\partial \mu_i}{\partial \eta_i}$. We have $\eta = X\beta$, where $X$ is a $n\times (p+1)$ matrix of observations. The score equation becomes 
\begin{equation*}
	X^\top DV^{-1}(y-\mu) = 0
\end{equation*}
In general, $D$ and $V$ may depend on $\mu$, and thus on $\beta$. Thus, an iterative method is required to solve for $\beta$. 
<a name="algo"></a><h3>A</h3>
<a name="fish"></a><h3>Fisher Information</h3>
Under regularity conditions, the MLE of $\hat{\beta}$ converges in distribution to a multivariate normal random variable with mean equal to the true $\beta$ and covariance matrix equal to the inverse of the Fisher information. 
\begin{equation*}
	E\left[-\frac{\partial^2 l(\beta;y_i)}{\partial\beta_h\partial\beta_j}\right]= E\left[\left(\frac{\partial l(\beta;y_i)}{\partial\beta_h}\right)\left(\frac{\partial l(\beta;y_i)}{\partial\beta_j}\right)\right] = \sum_{i=1}^n\frac{x_{ih}x_{ij}}{Var(y_i)}\left(\frac{\partial\mu_i}{\partial\eta_i}\right)^2
\end{equation*}
Let $W$ be a diagonal matric with diagonal elements 
\begin{equation*}
w_{ii}=\frac{\left(\frac{\partial\mu_i}{\partial\eta_i}\right)^2}{Var(y_i)}
\end{equation*}
The Fisher information matrix can be written as 
\begin{equation*}
	\mathcal{I}(\beta) = X^\top WX
\end{equation*}
	Thus, for large sample, $\beta \sim N[\beta,(X^\top WX)^{-1}]$.
<h4>Variance of the Estimators</h4>
By plugging in $\hat{\beta}$ into $\partial \mu_i/\partial \eta_i$ and $Var(y_i)$ to obtain $\hat{W}$, we can estimate the variance of $\hat{\beta}$
\begin{equation*}
\hat{Var}(\hat{\beta}) = (X^\top \hat{W}X)^{-1}
\end{equation*}
If $a(\phi) \neq 1$, then the variance can be estimated using maximum likelihood.
<a href="#delt"></a><h3>Delta Method</h3>
<a href="#robo"></a><h3>Robustness</h3>
<hr>-->
<a name="magf"></a><h2>Model Assessment and Goodness-of-Fit</h2>
<a name="LRT"></a><h3>Likelihood-Ratio Test (LRT)</h3>
For testing the hypothesis
\begin{align*}
&H_{0} : \beta = \beta_0\\
&H_{1} : \beta \neq \beta_0
\end{align*}
the statistic 
\begin{equation*}
	-2\log\Lambda = -2(l_1-l_0)
\end{equation*}
where $l_1$ and $l_0$ are the log-likelihood under $H_1$ and $H_0$ respectively, has assymptotic $\chi^2$ distribution with $dim(\beta)$ degrees of freedom, under regularity condition. 
<a name="wald"></a><h3>Wald's Test</h3>
For testing the hypothesis
\begin{align*}
&H_{0} : \beta_j = \beta_{j0}\\
&H_{1} : \beta_j \neq \beta_{j0}
\end{align*}
the statistic 
\begin{equation*}
	W = \frac{\hat{\beta}_j - \beta_{j0}}{\sqrt{Var(\beta)_{jj}}}
\end{equation*}
has assymptotic standard normal distribution. 
<h4>Multiple Parameters</h4>
For testing the hypothesis
\begin{align*}
&H_{0} : \beta_a = 0\\
&H_{1} : \beta_a \neq 0
\end{align*}
\begin{equation*}
	W = \hat{\beta}_a^\top[\hat{Var}(\hat{\beta}_a)]^{-1}\hat{\beta}_a
\end{equation*}
<a name="rao"></a><h3>Rao's Score Test</h3>
For testing the hypothesis
\begin{align*}
&H_{0} : \beta = \beta_0\\
&H_{1} : \beta \neq \beta_0
\end{align*}
the statistic 
\begin{equation*}
	S= \frac{\frac{\partial l(\beta;y)^2}{\partial \beta_0}}{-E\left[\frac{\partial^2 l(\beta;y)^2}{\partial \beta_0^2}\right]}
\end{equation*}
has assymptotic $\chi^2_1$ distribution.
<h4>Multiple Parameters</h4>
\begin{equation*}
	S = \frac{\partial l(\beta;y)^\top}{\partial \beta_0}\mathcal{I}(\hat{\beta}_0)\frac{\partial l(\beta;y)}{\partial \beta_0}
\end{equation*}
<a name="gofi"></a><h3>Goodness-of-Fit</h3>
Let $l(y,y)$ denote the ML for the <i>saturated model</i> and let $\tilde{\theta}_i$ be the estimate of $\theta_i$ with mean $\hat{\mu}_i$. The LRT goodness-of-fit statistic is 
\begin{equation*}
	-2[l(\hat{\mu}, y) - l(y,y)] = 2\sum_{i}[y_i(\tilde{\theta}_i) - b(\tilde{\theta}_i)]/a(\phi) -2\sum_{i}[y_i(\hat{\theta}_i) - b(\hat{\theta}_i)]/a(\phi)
\end{equation*}
If $a(\phi) = \phi/w_i$, then we obtain the <i>scaled deviance</i> 
\begin{equation*}
	2\sum_{i}w_i[y_i(\tilde{\theta}_i - \hat{\theta}_i) - b(\tilde{\theta}_i) + b(\hat{\theta}_i)]/\phi = D(y, \hat{\mu})/\phi
\end{equation*}
<h4>Deviance</h4>
The deviance of the model is denoted by $D(y,\hat{\mu})$, denoting the dfference in log-likelihoods between the proposed model and the saturated model. Large value indicate worse fit. This is the GLM equivalent of residual in OLS. 
<h4>Model Comparison</h4>
For $\phi = 1$, the deviance and scaled deviance are both equal to
\begin{equation*}
	D(y,\hat{\mu}) = -2[l(\hat{\mu};y) - l(y;y)]
\end{equation*}
<br><br>
Consider comparing two nester models, $M_0$ with $p_0$ parameter and fitted value $\hat{\mu}_0$, and $M_1$ with $p_1$ parameter and fitted value $\hat{\mu}_1$. 
\begin{align*}
	l(\hat{\mu}_0, y) \leq l(\hat{\mu}_1)\\
	D(y, \hat{\mu}_0) \leq D(y, \hat{\mu}_1)
\end{align*}
When comparing models, use the LRT statistic
\begin{align*}
	G^2(M_0|M_1) &:= -2[l(\hat{\mu}_0;y) - l(\hat{\mu}_1;y)] = D(y, \hat{\mu}_0) - D(y, \hat{\mu}_1) \\
	&= 2\sum_{i}w_i[y_i(\tilde{\theta}_{0i} - \hat{\theta}_{1i}) - b(\tilde{\theta}_{0i}) + b(\hat{\theta}_{1i})]
\end{align*}
Large values of the LRT statistic are equivalent to comparisons of lack of fit.
<h4>Beyond Deviance</h4>
If $a(\phi) = \phi = 1$, then the score statistic for comparing the chosen model with the saturated model can be written as
\begin{equation*}
	X^2 = \sum_{i=1}^n \frac{(y_i-\hat{\mu}_i)^2}{Var(\mu_i)}
\end{equation*}
<br><br>
For Poission model, this gives the usual Pearson $\chi^2$ statistic. 
<br><br>
Pearson $\chi^2$ statistic can be extended to compare two models. 
\begin{equation*}
	X^2(M_0|M_1) = \sum_{i=1}^n\frac{(\hat{\mu}_{0i} - \hat{\mu}_{1i})^2}{Var(\hat{\mu}_{0i})}
\end{equation*}
This has asymptotically equivalent behavior under the null hypothesis to $G^2(M_0|M_1)$, but it is not equal to the score statistic, as the score statistic is more complex.
<hr>
<a name="resi"></a><h3>Residual</h3>

<a name="infr"></a><h2>Inference</h2>
<a name="cfit"></a><h3>Confidence Interval</h3>
For any of these hypothesis tests for one parameter models, we can invert the test to obtain a 95% confidence interval.
<a name="plrt"></a><h3>Profile Log-Likelihood Ratio Test (PLRT)</h3>
For multi-parameter models, assume that we can partition $\beta$ into $(\beta_0, \psi)$ where $\beta_0$ is the target parameter of interest for the interval and $\psi$ is the set of nuisance parameters. For null hypothesis $H_0: \beta=\beta_0$, construct the original and profile log-likelihood
\begin{align*}
&l(\beta, \psi; y)\\
&l_1(\beta_0) = l_1(\beta_0, \hat{\psi}(\beta_0); y) = \max_{\psi}l(\beta_0, \hat{\psi}; y)
\end{align*}
The statistic 
\begin{equation*}
	-2[l_1(\beta_0, \hat{\psi}(\beta_0)) - l(\hat{\beta}, \hat{\psi})] \sim \chi_1^2
\end{equation*}
can be inverted to construct a confidence interval. 
<br><br>
Similarly, we can take the partial derivative of the profile log-likelihood to obtain a profile score function and construct an asymptotic profile score test and invert this score test to obtain a confidence interval.
<br><br>
The PLRT and the profile score tests are asymptotically equivalent except:
<ul>
	<li>The profile score interval requires computation of the profile Fisher information matrix, whereas the PLRT does not.</li>
	<li>The PLRT requires maximization of the original log-likelihood, whereas the profile score only requires maximization of the profile log-likelihood</li>
</ul>

<hr>
<a name="egmd"></a><h2>Relevant Models</h2>
<a name="logm"></a><h3>Probit Model</h3>
<a name="prom"></a><h3>Logistic Model</h3>
<a name="poim"></a><h3>Poission Model</h3>
<hr>
MATH 523 Generalized Linear Models, by Russell Steele<br>
McGill University, Winter 2015<br>

<div id="scrollTop">^<br>TOP</div>
<script>
		$("#scrollTop").click(function() {
		  $("html, body").animate({ scrollTop: 0 }, "fast");
		  return false;
		});
	</script>
</body>
</html>